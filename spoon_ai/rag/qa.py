from __future__ import annotations

import re
import os
from dataclasses import dataclass, field
from typing import Dict, List, Optional, TYPE_CHECKING, Any, Set

if TYPE_CHECKING:
    from spoon_ai.chat import Message  # type: ignore

from .config import RagConfig
from .retriever import RetrievedChunk


@dataclass
class Citation:
    source: str
    marker: str
    doc_id: Optional[str] = None
    chunk_index: Optional[int] = None
    text_snippet: Optional[str] = None


@dataclass
class QAResult:
    answer: str
    citations: List[Citation]
    raw_response: Optional[Any] = None


DEFAULT_QA_SYSTEM = (
    "You are a helpful assistant that answers questions using the provided context. "
    "Always cite sources using [n] markers (e.g. [1], [2]) that refer to the numbered context snippets provided."
)

QA_PROMPT_TEMPLATE = (
    "Answer the user question using only the context below.\n\n"
    "Context:\n{context}\n\n"
    "Question: {question}\n\n"
    "Instructions:\n"
    "- If the answer is not in the context, say you don't know.\n"
    "- Use [n] markers in the answer to cite the snippet numbers.\n"
    "- Keep the answer concise and relevant.\n"
)


class RagQA:
    def __init__(
        self,
        *,
        config: RagConfig,
        llm: Any,
        system_prompt: Optional[str] = None,
        user_template: Optional[str] = None,
    ):
        self.config = config
        self.llm = llm
        self.system_prompt = system_prompt or DEFAULT_QA_SYSTEM
        self.user_template = user_template or QA_PROMPT_TEMPLATE
        # Simple char limit safeguard (approx 30k tokens for modern models, but keep it safe)
        self.max_context_chars = 60000

    def _truncate_context(self, chunks: List[RetrievedChunk]) -> str:
        """Join chunks into a context string, respecting length limits."""
        lines = []
        current_len = 0
        
        for i, c in enumerate(chunks, start=1):
            # Format: [n] content...
            snippet = f"[{i}] {c.text}"
            snippet_len = len(snippet) + 2  # + 2 for newlines
            
            if current_len + snippet_len > self.max_context_chars:
                # Stop adding chunks if we exceed the budget
                break
                
            lines.append(snippet)
            current_len += snippet_len

        return "\n\n".join(lines)

    async def answer(self, question: str, chunks: List[RetrievedChunk]) -> QAResult:
        # P1: Handle empty chunks
        if not chunks:
            return QAResult(
                answer="I cannot answer this question because no relevant documents were found.",
                citations=[]
            )

        # Optional offline fallback
        if os.getenv("RAG_FAKE_QA") == "1" or not (self.llm and hasattr(self.llm, "ask")):
            # P2: Consistent language (English default) for offline fallback to match system prompt
            answer = "Offline Mode / No LLM:\n" + "\n".join([
                f"Source [{i}]: {c.text[:200]}..." for i, c in enumerate(chunks, start=1)
            ])
            cites = [
                Citation(
                    marker=f"[{i}]",
                    source=c.metadata.get("source", "unknown"),
                    doc_id=c.metadata.get("doc_id"),
                    chunk_index=c.metadata.get("chunk_index"),
                    text_snippet=c.text[:50]
                )
                for i, c in enumerate(chunks, start=1)
            ]
            return QAResult(answer=answer, citations=cites)

        # P0 & P1: Truncate and clean join
        context = self._truncate_context(chunks)
        prompt = self.user_template.format(context=context, question=question)

        # Lazy import to avoid circular dependency
        from spoon_ai.chat import Message  # type: ignore
        
        messages = [
            Message(role="system", content=self.system_prompt),
            Message(role="user", content=prompt),
        ]
        
        resp = await self.llm.ask(messages=messages)
        
        if isinstance(resp, str):
            text = resp
        else:
            text = getattr(resp, "content", "") or ""

        # P1: Regex-based citation parsing
        # Matches [1], [12], etc.
        found_indices: Set[int] = set()
        matches = re.findall(r"\[(\d+)\]", text)
        for m in matches:
            if m.isdigit():
                found_indices.add(int(m))

        final_citations: List[Citation] = []
        # chunks is 0-indexed, markers are 1-indexed
        for idx in sorted(found_indices):
            if 1 <= idx <= len(chunks):
                c = chunks[idx - 1]
                final_citations.append(
                    Citation(
                        marker=f"[{idx}]",
                        source=c.metadata.get("source", "unknown"),
                        doc_id=c.metadata.get("doc_id"),
                        chunk_index=c.metadata.get("chunk_index"),
                        text_snippet=c.text[:100]  # Store a bit of text for verification
                    )
                )

        return QAResult(answer=text, citations=final_citations, raw_response=resp)
